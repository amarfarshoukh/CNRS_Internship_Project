import asyncio
import json
import os
import re
import subprocess
import ast
from telethon import TelegramClient, events
from telethon.tl.types import Channel
import qrcode

# -----------------------------
# CONFIG
# -----------------------------
GEOJSON_FOLDER = r"C:\Users\user\OneDrive - Lebanese University\Documents\GitHub\Incident_Project\geojson_output_2"
OUTPUT_FILE = "matched_incidents.json"
OLLAMA_MODEL = "phi3:mini"
MAX_NUMBER_LEN = 6
api_id = 20976159
api_hash = '41bca65c99c9f4fb21ed627cc8f19ad8'
PHI3_TIMEOUT = 60

LOCATION_KEYWORDS = [
    # Longer / specific first
    "ุฅุชุฌุงู ุฃุทุฑุงู ุจูุฏุฉ", "ุฅุชุฌุงู ุจูุฏุฉ", "ูู ููุทูุฉ ุตูุงุนูุฉ", "ูู ููุทูุฉ", "ูู ูุญูุท", "ุนูู ุงูุชูุณุชุฑุงุฏ", 
    "ุนูู ุงูุญุฏูุฏ", "ูู ูุฏููุฉ", "ูู ูุฑูุฉ", "ูู ุจูุฏุฉ", "ูู ูุญูุฉ", "ูู ูุญุงูุธุฉ", "ูู ูุถุงุก", 
    "ุนูู ุทุฑูู", "ุนูู ุฌุณุฑ", "ุนูู ููุฑู", "ูู ุณุงุญุฉ", "ูู ุดุงุฑุน", "ูู ุณูู", "ูู ูุฎูู", 
    "ุจุงููุฑุจ ูู", "ุจุฌุงูุจ", "ุจุฌูุงุฑ", "ูุญุงุฐุงุฉ", "ููุงุจู", "ุจุฅุชุฌุงู", "ุฅุชุฌุงู", "ูุญู", 
    "ุฌููุจ", "ุดูุงู", "ุดุฑู", "ุบุฑุจ", "ุดูุงูู", "ุฌููุจู", "ุดุฑูู", "ุบุฑุจู", 
    "ุฅูู ุงูุฌููุจ", "ุฅูู ุงูุดูุงู", "ุฅูู ุงูุดุฑู", "ุฅูู ุงูุบุฑุจ",
    "ุนูุฏ", "ุฃุทุฑุงู", "ุจูุญูุท", "ุฏุงุฎู", "ุฎุงุฑุฌ", "ููุทูุฉ", "ูุญูุฉ", "ุจูุฏุฉ", "ูุฑูุฉ", "ูุฏููุฉ", "ุจูุงุฏ",
    "ูู", "ุจ", "ูู"
]

# Ensure longer keywords come first
LOCATION_KEYWORDS = sorted(LOCATION_KEYWORDS, key=len, reverse=True)


# -----------------------------
# Arabic normalization
# -----------------------------
RE_DIACRITICS = re.compile("[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]+")

def normalize_arabic(text: str) -> str:
    if not text:
        return ""
    text = RE_DIACRITICS.sub("", text)
    text = text.replace('\u0640', '')
    text = re.sub(r"[ุฅุฃุขุง]", "ุง", text)
    text = re.sub(r"[ุค]", "ู", text)
    text = re.sub(r"[ุฆ]", "ู", text)
    text = text.replace('ุฉ', 'ู')
    text = re.sub(r"[ูู]", "ู", text)
    return re.sub(r"\s+", " ", text).strip()

def is_arabic(text: str) -> bool:
    return bool(re.search(r'[\u0600-\u06FF]', text))

# -----------------------------
# Load GeoJSON locations
# -----------------------------
def extract_centroid(coords):
    if not coords:
        return None
    if isinstance(coords[0], list):
        points = coords[0] if isinstance(coords[0][0], list) else coords
    else:
        points = [coords]
    lon = sum([p[0] for p in points]) / len(points)
    lat = sum([p[1] for p in points]) / len(points)
    return [lon, lat]

def load_geojson_file(file_path):
    if not os.path.exists(file_path):
        return {}
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    features = data.get('features', data) if isinstance(data, dict) else data
    norm_map = {}
    for item in features:
        props = item.get('properties', item) if isinstance(item, dict) else item
        name = props.get('name') or item.get('name')
        coords = item.get('geometry', {}).get('coordinates') if 'geometry' in item else item.get('coordinates')
        if name and coords and is_arabic(name):
            try:
                centroid = extract_centroid(coords)
            except Exception:
                centroid = None
            norm_map[normalize_arabic(name)] = {"original": name, "coordinates": centroid}
    return norm_map

def load_all_geojson_folder(folder_path):
    all_map = {}
    total_features = 0
    for file in os.listdir(folder_path):
        if file.lower().endswith('.json'):
            file_path = os.path.join(folder_path, file)
            locations = load_geojson_file(file_path)
            count = len(locations)
            total_features += count
            print(f"โ Loaded {count} Arabic locations from {file}")
            all_map.update(locations)
    print("====================================")
    print(f"๐ TOTAL Arabic locations loaded: {total_features}")
    print("====================================")
    return all_map


ALL_LOCATIONS = load_all_geojson_folder(GEOJSON_FOLDER)
print(f"Loaded {len(ALL_LOCATIONS)} Arabic locations from GeoJSON folder")

# -----------------------------
# Location detection
# -----------------------------
def detect_location_from_map(text_norm):
    # Remove punctuation around words for robust matching
    words = re.sub(r'[^\wุก-ู]+', ' ', text_norm).split()
    
    for loc_norm, loc_data in ALL_LOCATIONS.items():
        loc_words = re.sub(r'[^\wุก-ู]+', ' ', loc_norm).split()
        for i in range(len(words) - len(loc_words) + 1):
            if words[i:i+len(loc_words)] == loc_words:
                return loc_data["original"], loc_data["coordinates"]
    return None, None

def detect_location(text):
    text_norm = normalize_arabic(text)
    words = text_norm.split()

    # Step 1: Multi-word location matches
    for loc_norm, loc_data in ALL_LOCATIONS.items():
        loc_words = loc_norm.split()
        if len(loc_words) > 1:
            for i in range(len(words) - len(loc_words) + 1):
                if words[i:i+len(loc_words)] == loc_words:
                    return loc_data["original"], loc_data["coordinates"]

    # Step 2: Single-word location matches
    for loc_norm, loc_data in ALL_LOCATIONS.items():
        loc_words = loc_norm.split()
        if len(loc_words) == 1 and loc_words[0] in words:
            return loc_data["original"], loc_data["coordinates"]

    # Step 3: Keyword fallback (optional)
    for kw in LOCATION_KEYWORDS:
        if kw in text_norm:
            for loc_norm, loc_data in ALL_LOCATIONS.items():
                if loc_norm.startswith(kw) or kw in loc_norm:
                    return loc_data["original"], loc_data["coordinates"]

    return None, None

# -----------------------------
# Incident keywords
# -----------------------------
class IncidentKeywords:
    def __init__(self):
        self.incident_keywords = {
            'vehicle_accident': ['ุงูุญุฑุงู ุดุงุญูุฉ','ุญุงุฏุซ','ุญุงุฏุซ ุณูุฑ', 'ุญูุงุฏุซ ุณูุฑ', 'ุชุตุงุฏู', 'ุชุตุงุฏู ุณูุงุฑุงุช', 'ุชุตุงุฏู ูุฑูุจุงุช', 'ุฏูุณ', 'ุญุงูุฉ ุฏูุณ', 'ุญูุงุฏุซ ุฏูุณ', 'ุงูููุงุจ', 'ุงูููุงุจ ุณูุงุฑุฉ', 'ุงูููุงุจ ูุฑูุจุฉ', 'ุงุตุทุฏุงู', 'ุงุตุทุฏุงู ุณูุงุฑุงุช', 'ุงุตุทุฏุงู ูุฑูุจุงุช', 'ุญุงุฏุซ ูุฑูุฑู', 'ุญูุงุฏุซ ูุฑูุฑูุฉ', 'ุญุงุฏุซ ุทุฑู', 'ุญูุงุฏุซ ุทุฑู', 'ุญุงุฏุซ ูุฑูุฑ', 'ุญูุงุฏุซ ูุฑูุฑ', 'ุชุตุงุฏู ูุฑูุฑู', 'ุญุงุฏุซ ุจุงุต', 'ุญุงุฏุซ ุดุงุญูุฉ', 'ุญุงุฏุซ ุฏุฑุงุฌุฉ', 'ุญุงุฏุซ ุฏุฑุงุฌุฉ ูุงุฑูุฉ'],
            'shooting': ['ุฅุทูุงู ูุงุฑ', 'ุฅุทูุงู ุงูุฑุตุงุต', 'ุฑุตุงุต', 'ุฑุตุงุตุฉ', 'ูุณูุญ', 'ูุณูุญูู', 'ูุฌูู ูุณูุญ', 'ูุฌูุงุช ูุณูุญุฉ', 'ุงุดุชุจุงู', 'ุงุดุชุจุงูุงุช', 'ุฅุทูุงู ุฃุนูุฑุฉ ูุงุฑูุฉ', 'ุฅุทูุงู ูุงุฑ ูุซูู', 'ุฅุทูุงู ูุงุฑ ุนุดูุงุฆู', 'ุฅุทูุงู ูุงุฑ ูุจุงุดุฑ', 'ุฅุทูุงู ูุงุฑ ูุชุจุงุฏู', 'ุฅุทูุงู ูุงุฑ ูู ุงูููุงุก', 'ุฅุทูุงู ูุงุฑ ุนูู ุชุฌูุน', 'ุฅุทูุงู ูุงุฑ ุนูู ุณูุงุฑุฉ', 'ุฅุทูุงู ูุงุฑ ุนูู ููุฒู', 'ุฅุทูุงู ูุงุฑ ุนูู ุฏูุฑูุฉ', 'ุฅุทูุงู ูุงุฑ ุนูู ุญุงุฌุฒ'],
            'protest': ['ุฎุทูุงุช ุชุตุนูุฏูุฉ','ุงุญุชุฌุงุฌ', 'ุงุญุชุฌุงุฌุงุช', 'ูุธุงูุฑุฉ', 'ูุธุงูุฑุงุช', 'ุชุธุงูุฑุฉ', 'ุชุธุงูุฑุงุช', 'ุงุนุชุตุงู', 'ุงุนุชุตุงูุงุช', 'ูุณูุฑุฉ ุงุญุชุฌุงุฌูุฉ', 'ูุณูุฑุงุช ุงุญุชุฌุงุฌูุฉ', 'ูุณูุฑุงุช', 'ุชุฌูุน ุงุญุชุฌุงุฌู', 'ุชุฌูุนุงุช ุงุญุชุฌุงุฌูุฉ', 'ููู ุงุญุชุฌุงุฌู', 'ูููุงุช ุงุญุชุฌุงุฌูุฉ', 'ุฅุถุฑุงุจ', 'ุฅุถุฑุงุจุงุช', 'ุชุฌููุฑ'],
            'fire': ['ุญุฑูู', 'ุงุญุชุฑุงู', 'ูุงุฑ', 'ุงุดุชุนุงู', 'ุงูุฏูุงุน', 'ุงูุฏูุงุน ุญุฑูู', 'ุฏุฎุงู', 'ุฏุฎุงู ูุซูู', 'ุชุตุงุนุฏ ุฏุฎุงู', 'ููุจ', 'ุฃูุณูุฉ ุงูููุจ', 'ุญุฑูู ูุจูุฑ', 'ุญุฑูู ุถุฎู', 'ุญุฑูู ูุงุฆู', 'ุญุฑูู ูุจูู', 'ุญุฑูู ููุฒู', 'ุญุฑูู ุบุงุจุฉ', 'ุงุดุชุนุงู ุงูููุฑุงู', 'ุงูุฏูุงุน ุงูููุฑุงู', 'ุงูุฏูุงุน ูุงุฑ'],
            'earthquake': ['ุฒูุฒุงู','ุฒูุงุฒู','ูุฒุฉ','ูุฒุงุช','ูุฒุฉ ุฃุฑุถูุฉ','ูุฒุงุช ุฃุฑุถูุฉ','ุฑุฌูุฉ','ุฑุฌูุงุช','ุฑุฌูุฉ ุฃุฑุถูุฉ','ุฑุฌูุงุช ุฃุฑุถูุฉ','ุงูุชุฒุงุฒ','ุงูุชุฒุงุฒุงุช','ุงุฑุชุฌุงุฌ','ุงุฑุชุฌุงุฌุงุช','ุงุฑุชุนุงุด ุฃุฑุถู','ุงุฑุชุฌุงู ุงูุฃุฑุถ','ุงูุดูุงู ุงูุฃุฑุถ','ุชุดูู ุงูุฃุฑุถ','ุตุฏุน ุฃุฑุถู','ุดุฑูุฎ ุฃุฑุถูุฉ'],
            'flood': ['ุชุณุฑุจ ููุงู','ููุถุงู','ููุถุงูุงุช','ุทููุงู','ุทููุงูุงุช','ุชุณููุงูู','ุณููู','ุณูู','ุงูุณููู','ุณููู ุฌุงุฑูุฉ','ููุถุงูุงุช ุฌุงุฑูุฉ','ุบูุฑ ูุงุฆู','ุชุฏูู ูุงุฆู','ุงุฑุชูุงุน ููุณูุจ ุงูููุงู','ุบุฑู ุงูุดูุงุฑุน','ุบุฑู ุงูุทุฑูุงุช','ุบุฑู ุงูููุงุฒู','ุงููุฌุงุฑ ุณุฏ','ุงูููุงุฑ ุณุฏ','ุฃูุทุงุฑ ุบุฒูุฑุฉ','ุบุฒุงุฑุฉ ุงูุฃูุทุงุฑ','ุชุฑุงูู ููุงู','ุชุฌููุน ููุงู','ุจุฑู ููุงู','ุจุญูุฑุงุช ูุคูุชุฉ','ุบูุฑ ุงูุฃุฑุงุถู ุงูุฒุฑุงุนูุฉ','ูุงุฑุซุฉ ูุงุฆูุฉ'],
            'tree_down': ['ุงูููุงุฑ','ุงูููุงุฑุงุช','ุงูููุงุฑ ุฃุฑุถู','ุงูููุงุฑุงุช ุฃุฑุถูุฉ','ุณููุท ุตุฎูุฑ','ุงูุฒูุงู ุตุฎูุฑ','ุงูุฒูุงู ุชุฑุจุฉ','ุงูุฒูุงูุงุช ุชุฑุจุฉ','ุงูููุงุฑ ุฌุจูู','ุงูููุงุฑุงุช ุฌุจููุฉ','ุณููุท ุดุฌุฑ','ุณููุท ุฃุดุฌุงุฑ','ุงูุชูุงุน ุดุฌุฑุฉ','ุงูุชูุงุน ุฃุดุฌุงุฑ','ุงูุชูุงุน ุฌุฐูุฑ','ุนุงุตูุฉ','ุนูุงุตู','ุนุงุตูุฉ ุฑุนุฏูุฉ','ุนูุงุตู ุฑุนุฏูุฉ','ุนุงุตูุฉ ุซูุฌูุฉ','ุนูุงุตู ุซูุฌูุฉ','ุนุงุตูุฉ ูุทุฑูุฉ','ุนูุงุตู ูุทุฑูุฉ','ุนุงุตูุฉ ููุงุฆูุฉ','ุนูุงุตู ููุงุฆูุฉ','ุนุงุตูุฉ ุชุฑุงุจูุฉ','ุนูุงุตู ุชุฑุงุจูุฉ','ุฑูุงุญ ูููุฉ','ุนูุงุตู ุฑูููุฉ','ุฅุนุตุงุฑ','ุฃุนุงุตูุฑ','ุฅุนุตุงุฑ ูุฏูุฑ','ุฅุนุตุงุฑ ููู','ุนุงุตูุฉ ูุฏุงุฑูุฉ','ุญุฑุงุฆู ุบุงุจุงุช','ุญุฑูู ุบุงุจุฉ','ุฌูุงู','ููุฌุฉ ุฌูุงู','ุชุณุงูุท ุงูุตุฎูุฑ','ุงูุญุฏุงุฑ ุตุฎุฑู','ููุฌุฉ ุนุงุชูุฉ','ููุฌุฉ ุฑูุงุญ','ุนุงุตูุฉ ูููุฉ','ุนุงุตูุฉ ูุฏูุฑุฉ'],
            'airstrike': ['ุชุญููู ููุณูุฑ','ุงุณุชูุฏุงู','ุงูุฃุณูุญุฉ ุงูุฑุดุงุดุฉ','ูุณูุฑุฉ', 'ุทูุฑุงู', 'ุญุฑุจู', 'ุทูุฑุงู ุญุฑุจู', 'ุบุงุฑุฉ', 'ุบุงุฑุฉ ุฌููุฉ', 'ูุตู', 'ูุตู ุฌูู', 'ูุตู ุตุงุฑูุฎู', 'ูุตู ูุฏูุนู', 'ุตุงุฑูุฎ', 'ุตูุงุฑูุฎ', 'ููุจูุฉ', 'ููุงุจู', 'ุทุงุฆุฑุฉ', 'ุทุงุฆุฑุงุช', 'ููุงุชูุฉ', 'ููุงุชูุงุช', 'ูุตู ุจุงูุทุงุฆุฑุงุช', 'ุถุฑุจุฉ ุฌููุฉ', 'ูุฌูู ุฌูู', 'ุชูุฌูุฑ ุฌูู', 'ุบุงุฑุฉ ุฌููุฉ ุฅุณุฑุงุฆูููุฉ', 'ุบุงุฑุฉ ุฅุณุฑุงุฆูููุฉ', 'ุณูุงุญ ุงูุฌู', 'ุถุฑุจุฉ ุตุงุฑูุฎูุฉ', 'ูุฌูู ุตุงุฑูุฎู', 'ูุฐููุฉ', 'ูุฐุงุฆู'],
            'collapse': ['ุงูููุงุฑ', 'ุงูููุงุฑ ูุจูู', 'ุงูููุงุฑุงุช','ุณููุท ูุจูู', 'ุณููุท ูุจุงูู', 'ุณููุท ุฌุฏุงุฑ', 'ุณููุท ุณูู', 'ุงูููุงุฑ ุณูู', 'ุงูููุงุฑ ุฌุฏุงุฑ', 'ุงูููุงุฑ ููุฒู', 'ุงูููุงุฑ ุนูุงุฑุฉ', 'ุงูููุงุฑ ุจูุงุก', 'ุงูููุงุฑ ุทุฑูู', 'ุงูููุงุฑ ุฌุณุฑ', 'ุงูููุงุฑ ุฃุฑุถู', 'ูุจูุท ุฃุฑุถู', 'ุชุตุฏุน', 'ุชุตุฏุนุงุช'],
            'pollution': ['ุงุนุชุฏุงุก', 'ุจูุฆู', 'ุงุนุชุฏุงุก ุจูุฆู', 'ุชููุซ', 'ุชุนุฏู', 'ุชุนุฏู ุนูู ุงูุจูุฆุฉ', 'ุชุณุฑูุจ', 'ููุทู', 'ุชุณุฑูุจ ููุทู', 'ุชููุซ ุงูููุงู', 'ุชููุซ ุงูููุงุก', 'ุชููุซ ููุงูุงุช', 'ููุจ', 'ููุจ ุนุดูุงุฆู', 'ููุงู ูููุซุฉ', 'ุตุฑู ุตุญู', 'ูุฌุงุฑูุฑ', 'ุฏุฎุงู', 'ุฏุฎุงู ุณุงู', 'ููุงูุงุช', 'ุชุณุฑุจ ููุงุฏ ููููุงุฆูุฉ', 'ุชุณุฑูุจ ููุงุฏ ุณุงูุฉ', 'ุชููุซ ุตูุงุนู', 'ุชููุซ ุฒุฑุงุนู', 'ููุงู ุขุณูุฉ', 'ุตุฑู ุตูุงุนู', 'ุชุณุฑุจ ููุท', 'ุชุณุฑุจ ูููุฏ', 'ุชุณุฑุจ ุบุงุฒ'],
            'epidemic': ['ูุจุงุก', 'ุชูุดู', 'ุชูุดู ูุจุงุก', 'ุงูุชุดุงุฑ ูุจุงุก', 'ุชูุดู ูุฑุถ', 'ูุฑุถ ูุนุฏ', 'ุฃูุฑุงุถ ูุนุฏูุฉ', 'ุฅุตุงุจุฉ ุฌูุงุนูุฉ', 'ุฅุตุงุจุงุช ุฌูุงุนูุฉ', 'ุนุฏูู', 'ุงูุชุดุงุฑ ุนุฏูู', 'ุญุงูุงุช ุนุฏูู', 'ุญุฌุฑ ุตุญู', 'ุญุงูุฉ ูุจุงุฆูุฉ', 'ุญุงูุงุช ูุจุงุฆูุฉ', 'ุญุงูุฉ ุทูุงุฑุฆ ุตุญูุฉ', 'ุญุฌุฑ ุตุญู ุฌูุงุนู', 'ุงูุชุดุงุฑ ูุฑุถ'],
            'medical': ['ุฏูุงุก','ุฏู','ุฅุณุนุงู', 'ุงุณุนุงู', 'ูุณุชุดูู', 'ูุณุชุดููุงุช', 'ุทูุงุฑุฆ', 'ูุณู ุงูุทูุงุฑุฆ', 'ุฅูุนุงุด', 'ุงูุนุงุด', 'ุณูุงุฑุฉ ุฅุณุนุงู', 'ุณูุงุฑุงุช ุฅุณุนุงู', 'ุฎุฏูุฉ ุทุจูุฉ', 'ุฎุฏูุงุช ุทุจูุฉ', 'ุฅุตุงุจุฉ ุทุจูุฉ', 'ุฅุตุงุจุงุช ุทุจูุฉ', 'ูุฑูู ุทุจู', 'ุทุจูุจ', 'ุฃุทุจุงุก', 'ููุฑุถ', 'ููุฑุถุฉ', 'ุทุงูู ุทุจู', 'ุนูุงุฌ', 'ุฑุนุงูุฉ ุทุจูุฉ', 'ุญุงูุฉ ุตุญูุฉ', 'ุญุงูุงุช ุญุฑุฌุฉ', 'ูุตุงุจ', 'ูุตุงุจูู', 'ููู ุทุจู', 'ุฅุฎูุงุก ุทุจู', 'ุฅุณุนุงูุงุช ุฃูููุฉ','ุฌุฑูุญ' 'ุฌุฑุญู', 'ูุตุงุจ', 'ูุตุงุจูู', 'ูุตุงุจูู', 'ุฅุตุงุจุฉ', 'ุฅุตุงุจุงุช', 'ุฅุตุงุจุงุช ุญุฑุฌุฉ', 'ุฅุตุงุจุงุช ุจุงูุบุฉ', 'ุฅุตุงุจุงุช ุทูููุฉ', 'ุฅุตุงุจุฉ ุดุฎุต', 'ุฅุตุงุจุฉ ุฃุดุฎุงุต', 'ุฅุตุงุจุฉ ูุฏูู', 'ุฅุตุงุจุฉ ูุฏูููู', 'ุฅุตุงุจุฉ ุทูู', 'ุฅุตุงุจุฉ ุฃุทูุงู', 'ุฅุตุงุจุฉ ุงูุฑุฃุฉ', 'ุฅุตุงุจุฉ ูุณุงุก'],
            'explosion': ['ุงููุฌุงุฑ', 'ุชูุฌูุฑ', 'ุนุจูุฉ', 'ูุงุณูุฉ', 'ุนุจูุฉ ูุงุณูุฉ', 'ุงููุฌุงุฑุงุช', 'ุชูุฌูุฑุงุช', 'ููุจูุฉ', 'ููุงุจู', 'ุงููุฌุงุฑ ููู', 'ุงููุฌุงุฑ ุนููู', 'ุงููุฌุงุฑ ุนุจูุฉ', 'ุงููุฌุงุฑ ุณูุงุฑุฉ ููุฎุฎุฉ', 'ุณูุงุฑุฉ ููุฎุฎุฉ', 'ุงููุฌุงุฑ ุฐุฎูุฑุฉ', 'ุงููุฌุงุฑ ูุบู', 'ุงููุฌุงุฑ ุบุงูุถ', 'ุฏูู ุงููุฌุงุฑ', 'ุฏูู ููู', 'ุงููุฌุงุฑ ุตูุชู', 'ุงููุฌุงุฑ ููุฒู', 'ุงููุฌุงุฑ ูุจูู'],
        }
        self.casualty_keywords = {
            'killed': ['ูุชูู', 'ูุชูู', 'ุดูุฏุงุก', 'ุดููุฏ', 'ููุงุฉ', 'ูููุงุช', 'ููุชู', 'ููุชู ุดุฎุต', 'ููุชู ุฃุดุฎุงุต', 'ููุชู ูุฏูู', 'ููุชู ูุฏูููู', 'ููุชู ุทูู', 'ููุชู ุฃุทูุงู', 'ููุชู ุงูุฑุฃุฉ', 'ููุชู ูุณุงุก', 'ูุชู', 'ูุชูู ุงูุญุงุฏุซ', 'ุณููุท ูุชูู', 'ุณููุท ุถุญุงูุง', 'ุถุญุงูุง', 'ุถุญุงูุง ุงููุชู'],
            'injured': ['ุฌุฑูุญ', 'ุฌุฑุญู', 'ูุตุงุจ', 'ูุตุงุจูู', 'ูุตุงุจูู', 'ุฅุตุงุจุฉ', 'ุฅุตุงุจุงุช', 'ุฅุตุงุจุงุช ุญุฑุฌุฉ', 'ุฅุตุงุจุงุช ุจุงูุบุฉ', 'ุฅุตุงุจุงุช ุทูููุฉ', 'ุฅุตุงุจุฉ ุดุฎุต', 'ุฅุตุงุจุฉ ุฃุดุฎุงุต', 'ุฅุตุงุจุฉ ูุฏูู', 'ุฅุตุงุจุฉ ูุฏูููู', 'ุฅุตุงุจุฉ ุทูู', 'ุฅุตุงุจุฉ ุฃุทูุงู', 'ุฅุตุงุจุฉ ุงูุฑุฃุฉ', 'ุฅุตุงุจุฉ ูุณุงุก'],
            'missing': ['ููููุฏ', 'ููููุฏูู', 'ููููุฏุฉ', 'ููููุฏุงุช', 'ุงุฎุชูู', 'ุงุฎุชูุงุก', 'ููุฏุงู', 'ุญุงูุฉ ููุฏุงู', 'ุจูุงุบ ููุฏุงู', 'ููููุฏ ุงูุดุฎุต', 'ููููุฏุฉ ุงูุดุฎุต']
        }

    def extract_casualties(self, text):
        tl = text.lower()
        cats = []
        for cat, kws in self.casualty_keywords.items():
            for kw in kws:
                if kw in tl:
                    cats.append(cat)
        return list(set(cats))

    def extract_numbers(self, text):
        nums = re.findall(r"[0-9]+|[ู-ูฉ]+", text)
        conv = str.maketrans("ููกูขูฃูคูฅูฆูงูจูฉ", "0123456789")
        cleaned = [n.translate(conv) for n in nums if len(n.translate(conv)) <= MAX_NUMBER_LEN]
        return cleaned

IK = IncidentKeywords()

# -----------------------------
# Incident detection helper (multi)
# -----------------------------
def find_incident_types(text, incident_keywords):
    if not text:
        return []
    norm_text = normalize_arabic(text)
    found = []
    for inc_type, keywords in incident_keywords.items():
        for kw in keywords:
            if normalize_arabic(kw) in norm_text:
                found.append(inc_type)
    return list(set(found))

# -----------------------------
# Robust Phi3 JSON Extractor
# -----------------------------
def robust_json_extract(text):
    if not text:
        return None
        
    text = re.sub(r'```json|```', '', text).strip()
    text = re.sub(r'^"{3,}', '', text).strip()
    text = re.sub(r'"{3,}$', '', text).strip()
    start = text.find('{')
    end = text.rfind('}')
    if start == -1 or end == -1 or end <= start:
        return None
    json_str = text[start:end+1]
    json_str = re.sub(r'//.*', '', json_str)
    json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
    try:
        data = json.loads(json_str)
        # Force incident_type as list
        itype = data.get("incident_type")
        if itype:
            if not isinstance(itype, list):
                data["incident_type"] = [itype]
        else:
            data["incident_type"] = []
        if "threat_level" in data and data["threat_level"] not in ["yes", "no"]:
            data["threat_level"] = "yes"
        return data
    except Exception:
        return None

# -----------------------------
# Phi3 JSON query
# -----------------------------
def query_phi3_json(message: str):
    if not message:
        return None
        
    prompt = f"""
You are an incident analysis assistant.
Return ONLY valid JSON. Do NOT include any explanations.
{{"location": ..., "incident_type": ..., "threat_level": ..., "casualties": [...], "numbers": [...]}}

Message: "{message}"
"""

    try:
        res = subprocess.run(
            ["ollama", "run", OLLAMA_MODEL],
            input=prompt.encode("utf-8"),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=PHI3_TIMEOUT
        )
        text = res.stdout.decode("utf-8", errors="ignore").strip()
        return robust_json_extract(text)
    except Exception as e:
        print("Phi3 call failed:", e)
        return None

# -----------------------------
# Load/save matches
# -----------------------------
def load_existing_matches(path=OUTPUT_FILE):
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading existing matches: {e}")
            return []
    return []

def save_matches(matches, path=OUTPUT_FILE):
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(matches, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Error saving matches: {e}")

# -----------------------------
# Deduplication
# -----------------------------
def select_best_message(records):
    if not records:
        return None
    records.sort(key=lambda m: (
        len(m.get('details', {}).get('numbers_found', [])) +
        len(m.get('details', {}).get('casualties', [])) +
        len(m.get('details', {}).get('summary', ''))
    ), reverse=True)
    return records[0]

# -----------------------------
# Clean summary helper
# -----------------------------
def clean_summary(text: str) -> str:
    if not text:
        return ""
    # 1. Remove URLs
    text = re.sub(r'http\S+', '', text)

    # 2. Remove Unicode control characters
    text = re.sub(r'[\u200B-\u200F\u202A-\u202E\u2066-\u2069]', '', text)

    # 3. Remove escaped newlines/quotes
    text = text.replace("\\n", " ").replace("\n", " ").replace("\"", " ")

    # 4. Keep only Arabic, English, digits, punctuation
    text = re.sub(r'[^ุก-ูa-zA-Z0-9\s\.,ุุ:!ุ-]', '', text)

    # 5. Normalize spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# -----------------------------
# Phi3 worker queue (multi-incident)
# -----------------------------
message_queue = asyncio.Queue()

async def phi3_worker(matches, existing_ids):
    VALID_INCIDENT_TYPES = set(IK.incident_keywords.keys())

    while True:
        try:
            event = await message_queue.get()
            text = event.raw_text or ""
            channel_name = (
                event.chat.username if event.chat and getattr(event.chat, 'username', None)
                else str(event.chat_id)
            )
            msg_id = event.id

            # Skip already processed messages
            if (channel_name, msg_id) in existing_ids:
                message_queue.task_done()
                continue

            # --- Location detection using map
            location, coordinates = detect_location(text)

            # --- Incident type detection (keywords first)
            incident_types = find_incident_types(text, IK.incident_keywords)

            # --- Fallback to Phi3 if keywords fail or location not found
            phi3_res = None
            if not incident_types or not location:
                phi3_res = query_phi3_json(text)

                if phi3_res:
                    # Incident type from Phi3
                    if not incident_types:
                        itype = phi3_res.get("incident_type")
                        if itype:
                            if isinstance(itype, list):
                                incident_types = [i for i in itype if i in VALID_INCIDENT_TYPES]
                            elif isinstance(itype, str) and itype in VALID_INCIDENT_TYPES:
                                incident_types = [itype]

                    # Location from Phi3 (strict map validation)
                    if not location:
                        phi3_loc = phi3_res.get("location")
                        loc_candidates = []

                        if isinstance(phi3_loc, str):
                            loc_candidates.append(phi3_loc)
                        elif isinstance(phi3_loc, list):
                            loc_candidates.extend([l for l in phi3_loc if isinstance(l, str)])
                        elif isinstance(phi3_loc, dict):
                            for key in ["city", "town", "location", "name"]:
                                loc_val = phi3_loc.get(key)
                                if isinstance(loc_val, str):
                                    loc_candidates.append(loc_val)

                        # Accept only if exists in map and text mentions it
                        for loc in loc_candidates:
                            loc_norm = normalize_arabic(loc)
                            if loc_norm in ALL_LOCATIONS:
                                text_norm = normalize_arabic(text)
                                loc_words = loc_norm.split()
                                text_words = text_norm.split()
                                for i in range(len(text_words) - len(loc_words) + 1):
                                    if text_words[i:i+len(loc_words)] == loc_words:
                                        location = ALL_LOCATIONS[loc_norm]["original"]
                                        coordinates = ALL_LOCATIONS[loc_norm]["coordinates"]
                                        break
                                if location:
                                    break

            # --- Skip if no valid incident type or location/coordinates
            if not incident_types or not location or not coordinates:
                print(f"[SKIP] {text[:50]}... (no valid incident/location)")
                message_queue.task_done()
                continue

            # --- Extract numbers and casualties
            numbers = IK.extract_numbers(text)
            casualties = IK.extract_casualties(text)

            # --- Clean summary
            summary = clean_summary(text)
            if len(summary) > 300:
                summary = summary[:300] + "..."

            # --- Create records for each incident type
            for incident_type in incident_types:
                if incident_type not in VALID_INCIDENT_TYPES:
                    continue  # skip invalid Phi3 types

                record = {
                    "incident_type": incident_type,
                    "location": location,
                    "coordinates": coordinates,
                    "channel": channel_name,
                    "message_id": msg_id,
                    "date": str(event.date),
                    "threat_level": "yes",
                    "details": {
                        "numbers_found": numbers,
                        "casualties": casualties,
                        "summary": summary
                    }
                }

                # --- Skip duplicates by message_id
                if not any(m.get("message_id") == msg_id for m in matches):
                    matches.append(record)
                    print(f"[MATCH] {incident_type} @ {location} from {channel_name}")
                    save_matches(matches)

            existing_ids.add((channel_name, msg_id))

        except Exception as e:
            print(f"Error processing message: {e}")
        finally:
            if not message_queue.empty():
                message_queue.task_done()

# -----------------------------
# Telegram login
# -----------------------------
async def qr_login(client):
    if not await client.is_user_authorized():
        print("Scan QR code:")
        qr_login_obj = await client.qr_login()
        qr = qrcode.QRCode()
        qr.add_data(qr_login_obj.url)
        qr.make()
        qr.print_ascii(invert=True)
        await qr_login_obj.wait()

async def get_my_channels(client):
    out = []
    async for d in client.iter_dialogs():
        if isinstance(d.entity, Channel):
            out.append(d.entity)
    return out

# -----------------------------
# Main async
# -----------------------------
async def main():
    client = TelegramClient('session', api_id, api_hash)
    await client.start()
    await qr_login(client)
    channels = await get_my_channels(client)
    channel_ids = [c.id for c in channels]
    print(f"Monitoring {len(channel_ids)} channels...")

    matches = load_existing_matches()
    existing_ids = {(m.get('channel'), m.get('message_id')) for m in matches}

    # Start multiple workers for better performance
    workers = []
    for _ in range(3):  # Start 3 workers
        workers.append(asyncio.create_task(phi3_worker(matches, existing_ids)))

    @client.on(events.NewMessage(chats=channel_ids))
    async def handler(event):
        await message_queue.put(event)

    print("Started monitoring. Waiting for new messages...")
    try:
        await asyncio.gather(*workers)
    except asyncio.CancelledError:
        print("Shutting down...")
    finally:
        # Cancel all worker tasks
        for worker in workers:
            worker.cancel()
        # Wait for all tasks to complete
        await asyncio.gather(*workers, return_exceptions=True)
        await client.disconnect()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Interrupted by user")